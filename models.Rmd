---
title: "models"
output: html_document
date: "2024-11-22"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r load-lib, message = FALSE, echo = FALSE}
library(tidyverse)
library(readr)
library(dplyr)
library(janitor)
library(tidymodels)

```



```{r load-data, echo=FALSE}
olympics <- read_csv("data/olympics.csv", show_col_types = FALSE)

olympic_medals <- olympics %>% 
  filter(!is.na(medal)) %>%
  mutate(
   host_country = case_when(
     city == "Albertville" ~ "FRA",
     city == "Amsterdam" ~ "NED",
     city == "Antwerpen" ~ "BEL",
     city == "Athina" ~ "GRE",
     city == "Atlanta" ~ "USA", 
     city == "Barcelona" ~ "ESP",
     city == "Beijing" ~ "CHN",
     city == "Berlin" ~ "GER",
     city == "Calgary" ~ "CAN",
     city == "Chamonix" ~ "FRA",
     city == "Cortina d'Ampezzo" ~ "ITA",
     city == "Garmisch-Partenkirchen" ~ "GER",
     city == "Grenoble" ~ "FRA",
     city == "Helsinki" ~ "FIN",
     city == "Innsbruck" ~ "AUT",
     city == "Lake Placid" ~ "USA",
     city == "Lillehammer" ~ "NOR",
     city == "London" ~ "GBR",
     city == "Los Angeles" ~ "USA",
     city == "Melbourne" ~ "AUS",
     city == "Mexico City" ~ "MEX",
     city == "Montreal" ~ "CAN",
     city == "Moskva" ~ "RUS",
     city == "Munich" ~ "GER",
     city == "Nagano" ~ "JPN",
     city == "Oslo" ~ "NOR",
     city == "Paris" ~ "FRA",
     city == "Rio de Janeiro" ~ "BRA",
     city == "Roma" ~ "ITA",
     city == "Salt Lake City" ~ "USA",
     city == "Sankt Moritz" ~ "SUI",
     city == "Sapporo" ~ "JPN",
     city == "Sarajevo" ~ "BIH",
     city == "Seoul" ~ "KOR",
     city == "Sochi" ~ "RUS",
     city == "Squaw Valley" ~ "USA",
     city == "St. Louis" ~ "USA",
     city == "Stockholm" ~ "SWE",
     city == "Sydney" ~ "AUS",
     city == "Tokyo" ~ "JPN",
     city == "Torino" ~ "ITA",
     city == "Vancouver" ~ "CAN"
     )) 
```


### General Functions

```{r}


predictors <- c("year", "num_athletes", "medals_before", "is_host", "avg_age", "avg_height", "avg_weight")

# Compute all subsets of a set excluding the empty one
powerset <- function(set) {
  n <- length(set)
  subsets <- unlist(lapply(1:n, function(k) combn(set, k, simplify = FALSE)), recursive = FALSE)
  
  subsets <- subsets[sapply(subsets, length) > 0]
  
  return(subsets)
}


all_subsets <- powerset(predictors)

```

```{r}

# Fit the model on a single set of predictors, return adjusted R-squared and MAE
fit_model_for_subset <- function(subset, train_data, test_data) {
  # Check if the subset is empty
  if (length(subset) == 0) {
    return("Empty set of predictors not allowed") 
  }
  
  # Create formula dynamically from the subset
  formula <- as.formula(paste("total_medals ~", paste(subset, collapse = " + ")))
  
  # Define the recipe
  recipe_sub <- recipe(formula, data = train_data) %>%
    step_dummy(all_nominal()) %>%   # Create dummy variables for factors
    step_normalize(all_numeric_predictors())  
  
  # Define the linear regression model
  model <- linear_reg() %>%
    set_engine("lm")
  
  # Create the workflow
  workflow_sub <- workflow() %>%
    add_recipe(recipe_sub) %>%
    add_model(model)
  
  # Fit the model
  fit_sub <- fit(workflow_sub, data = train_data)
  
  # Make predictions
  pred_sub <- predict(fit_sub, test_data) %>%
    bind_cols(test_data) %>%
    mutate(.pred = round(.pred)) %>%
    mutate(.pred = if_else(.pred < 0, 0, .pred))
  
  # Extract adjusted R-squared and MAE
  fit_summary <- glance(fit_sub)
  adj_r_squared <- fit_summary$adj.r.squared
  mae_value <- mae(pred_sub, truth = total_medals, estimate = .pred)$.estimate
  
  # Return both MAE and adjusted R-squared as a named list
  return(list(mae = mae_value, adj_r_squared = adj_r_squared))
}

# Apply fit_model_for_subset to all combinations of predictors
evaluate_all_subsets <- function(all_subsets, train_data, test_data) {
  # Initialize an empty list to store results
  results <- list()
  
  # Loop through all subsets
  for (subset in all_subsets) {
    if (length(subset) == 0) {
      # Skip empty subsets
      next
    }
    
    # Fit the model and extract metrics
    metrics <- fit_model_for_subset(subset, train_data, test_data)
    
    # Store the subset, MAE, and adjusted R-squared in the results
    results[[length(results) + 1]] <- list(
      subset = paste(subset, collapse = ", "),
      mae = metrics$mae,
      adj_r_squared = metrics$adj_r_squared
    )
  }
  
  # Convert the list to a data frame
  results_df <- do.call(rbind, lapply(results, as.data.frame))
  
  # Convert MAE and adjusted R-squared to numeric 
  results_df$mae <- as.numeric(results_df$mae)
  results_df$adj_r_squared <- as.numeric(results_df$adj_r_squared)
  
  # Arrange by ascending MAE
  results_df <- results_df[order(results_df$mae), ]
  
  # Return the sorted results
  return(results_df)
}


```



### Summer Olympics

* exclude 2016 Summer Olympics, use it later to test
```{r , echo=FALSE}
# Prepare data
medals_summer <- olympic_medals %>%
  filter(season == "Summer") %>%
  group_by(year) %>%  # Group by year
  mutate(
    age = ifelse(is.na(age), mean(age, na.rm = TRUE), age),
    weight = ifelse(is.na(weight), mean(weight, na.rm = TRUE), weight),
    height = ifelse(is.na(height), mean(height, na.rm = TRUE), height)
  ) %>%
  ungroup() %>%
  group_by(year, noc) %>% # Group by year and team (country)
  summarise(
    total_medals = n(), # Total medals won
    num_athletes = n_distinct(id), # Number of unique athletes
    avg_age = mean(age), # Average age of athletes
    avg_height = mean(height), # Average height
    avg_weight = mean(weight), # Average weight
    is_host = if_else(any(noc == host_country), 1, 0), # Check if NOC is the host
    .groups = 'drop'
  ) %>%
  arrange(year) %>% # Arrange by year for cumulative calculation
  group_by(noc) %>%
  mutate(
    medals_before = cumsum(total_medals) - total_medals) %>%
  ungroup() %>%
  mutate(is_host = as.factor(is_host)) # Convert is_host to factor


# Remove 2016 to test on it separately
medals_summer_new <- medals_summer %>%
  filter(year!="2016")

# Split data into training and testing sets
set.seed(1114)
split_summer <- initial_split(medals_summer_new, prop = 0.8)
train_summer <- training(split_summer)
test_summer <- testing(split_summer)


```



* get top performing models, arranged by the value of MAE

```{r}

# Apply evaluate_all_subsets() to all sets of predictors
results_summer <- evaluate_all_subsets(all_subsets, train_summer, test_summer)
```



* examine model with all predictors

```{r}

model <- linear_reg() %>%
  set_engine("lm")


recipe_s1 <- recipe(total_medals ~ num_athletes + medals_before + avg_age + avg_height + avg_weight + year + is_host,
                    data = train_summer) %>%
  step_dummy(all_nominal()) %>%
  step_normalize(all_numeric_predictors())


workflow_s1 <- workflow() %>%
  add_recipe(recipe_s1) %>%
  add_model(model)

fit_s1 <- fit(workflow_s1, data = train_summer)

tidy(fit_s1)
glance(fit_s1)


pred_s1 <- predict(fit_s1, test_summer) %>%
  bind_cols(test_summer) %>%
  mutate(.pred = round(.pred)) %>%
  mutate(.pred = if_else(.pred < 0, 0, .pred))

ggplot(pred_s1, aes(x = total_medals, y = .pred)) +
  geom_point(color = "blue") +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(
    title = "Actual vs Predicted Total Medals",
    x = "Actual Medals",
    y = "Predicted Medals"
  ) +
  theme_minimal()

pred_s1

rmse(pred_s1, truth = total_medals, estimate = .pred)
mae(pred_s1, truth = total_medals, estimate = .pred)
```


* can do even better
* examine best model based on MAE

```{r}
model <- linear_reg() %>%
  set_engine("lm")


recipe_s2 <- recipe(total_medals ~  num_athletes + medals_before + avg_height + is_host,
                    data = train_summer) %>%
  step_dummy(all_nominal()) %>%
  step_normalize(all_numeric_predictors())


workflow_s2 <- workflow() %>%
  add_recipe(recipe_s2) %>%
  add_model(model)

fit_s2 <- fit(workflow_s2, data = train_summer)

tidy(fit_s2)
glance(fit_s2)


pred_s2 <- predict(fit_s2, test_summer) %>%
  bind_cols(test_summer) %>%
  mutate(.pred = round(.pred)) %>%
  mutate(.pred = if_else(.pred < 0, 0, .pred))

ggplot(pred_s2, aes(x = total_medals, y = .pred)) +
  geom_point(color = "blue") +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(
    title = "Actual vs Predicted Total Medals",
    x = "Actual Medals",
    y = "Predicted Medals"
  ) +
  theme_minimal()

pred_s2

rmse(pred_s2, truth = total_medals, estimate = .pred)
mae(pred_s2, truth = total_medals, estimate = .pred)

results_summer %>%
  filter(mae < 2)%>%
  count()
```
* note that top 2 have MAE = 1.81 to 3 sf
* top 22 - 1.8 to 2 sf
* top 64 - 2 to 1 sf
* no big differences, so best models will give pretty much the same values for predicted number of medals (differences likely to be more visible when number of medals is low)
* half of the models have average deviation from the real number of total medals won of 2 medals




* compare predicted and actual total medals using 2016 Summer Olympics

```{r}
summer_2016 <- medals_summer %>%
  filter(year == "2016")

pred_2016 <- predict(fit_s2, summer_2016) %>%
  bind_cols(summer_2016) %>%
  mutate(.pred = round(.pred)) %>%
  mutate(.pred = if_else(.pred < 0, 0, .pred)) %>%     
  select(noc, total_medals, .pred) %>%          
  arrange(desc(total_medals)) %>%
  mutate(pct_error = ((abs(total_medals - .pred))/total_medals) * 100)

pred_2016 %>% slice_head(n = 10)

pred_2016 %>%
  ggplot(mapping = aes(x = .pred, y = total_medals)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(
    title = "Actual vs Predicted Total Medals",
    x = "Actual Medals",
    y = "Predicted Medals"
  ) +
  theme_minimal()

pred_2016 %>%
  filter(total_medals > 25) %>%
  summarise(avg_error = mean(pct_error))
 
rmse(pred_2016, truth = total_medals, estimate = .pred)
mae(pred_2016, truth = total_medals, estimate = .pred)

pred_2016 %>%
  summarise(avg = mean(total_medals))

```

* largest errors when few medals
* for teams with more than 25 medal, average error is 6.8%
* MAE is 1.8, or 7.8% of the mean (23.5), which is a reasonable fit




### Winter Olympics

```{r , echo=FALSE}

# Prepare data
medals_winter <- olympic_medals %>%
  filter(season == "Winter") %>%
  group_by(year) %>%  # Group by year
  mutate(
    age = ifelse(is.na(age), mean(age, na.rm = TRUE), age),
    weight = ifelse(is.na(weight), mean(weight, na.rm = TRUE), weight),
    height = ifelse(is.na(height), mean(height, na.rm = TRUE), height)
  ) %>%
  ungroup() %>%
  group_by(year, noc) %>% # Group by year and team (country)
  summarise(
    total_medals = n(), # Total medals won
    num_athletes = n_distinct(id), # Number of unique athletes
    avg_age = mean(age), # Average age of athletes
    avg_height = mean(height), # Average height
    avg_weight = mean(weight), # Average weight
    is_host = if_else(any(noc == host_country), 1, 0), # Check if NOC is the host
    .groups = 'drop'
  ) %>%
  arrange(year) %>% # Arrange by year for cumulative calculation
  group_by(noc) %>%
  mutate(
    medals_before = cumsum(total_medals) - total_medals) %>%
  ungroup() %>%
  mutate(is_host = as.factor(is_host)) # Convert is_host to factor


# Remove 2014 to test on it separately
medals_winter_new <- medals_winter %>%
  filter(year!="2014")

# Split data into training and testing sets
set.seed(1114)
split_winter <- initial_split(medals_winter_new, prop = 0.8)
train_winter <- training(split_winter)
test_winter <- testing(split_winter)

```



* get top performing models, arranged by the value of MAE

```{r}

# Apply evaluate_all_subsets() to all subsets
results_winter <- evaluate_all_subsets(all_subsets, train_winter, test_winter) 

```



* examine model with all predictors

```{r}

model <- linear_reg() %>%
  set_engine("lm")


recipe_w1 <- recipe(total_medals ~ num_athletes + medals_before + avg_age + avg_height + avg_weight + year + is_host,
                    data = train_winter) %>%
  step_dummy(all_nominal()) %>%
  step_normalize(all_numeric_predictors())


workflow_w1 <- workflow() %>%
  add_recipe(recipe_w1) %>%
  add_model(model)

fit_w1 <- fit(workflow_w1, data = train_winter)

tidy(fit_w1)
glance(fit_w1)


pred_w1 <- predict(fit_w1, test_winter) %>%
  bind_cols(test_winter) %>%
  mutate(.pred = round(.pred)) %>%
  mutate(.pred = if_else(.pred < 0, 0, .pred))

ggplot(pred_w1, aes(x = total_medals, y = .pred)) +
  geom_point(color = "blue") +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(
    title = "Actual vs Predicted Total Medals",
    x = "Actual Medals",
    y = "Predicted Medals"
  ) +
  theme_minimal()

pred_w1

rmse(pred_w1, truth = total_medals, estimate = .pred)
mae(pred_w1, truth = total_medals, estimate = .pred)
```



* can do even better
* examine best model based on mae

```{r}

model <- linear_reg() %>%
  set_engine("lm")


recipe_w2 <- recipe(total_medals ~ num_athletes + avg_weight + avg_height + year + medals_before,
                    data = train_winter) %>%
  step_dummy(all_nominal()) %>%
  step_normalize(all_numeric_predictors())


workflow_w2 <- workflow() %>%
  add_recipe(recipe_w2) %>%
  add_model(model)

fit_w2 <- fit(workflow_w2, data = train_winter)

tidy(fit_w2)
glance(fit_w2)


pred_w2 <- predict(fit_w2, test_winter) %>%
  bind_cols(test_winter) %>%
  mutate(.pred = round(.pred)) %>%
  mutate(.pred = if_else(.pred < 0, 0, .pred))

ggplot(pred_w2, aes(x = total_medals, y = .pred)) +
  geom_point(color = "blue") +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(
    title = "Actual vs Predicted Total Medals",
    x = "Actual Medals",
    y = "Predicted Medals"
  ) +
  theme_minimal()

pred_w2

rmse(pred_w2, truth = total_medals, estimate = .pred)
mae(pred_w2, truth = total_medals, estimate = .pred)


results_winter %>%
  filter(mae < 2)%>%
  count()
```
* note that top 2 have MAE = 1.61 to 3 sf
* top 7 - 1.6 to 2 sf
* top 64 - 2 to 1 sf
* no big differences, so best models will give pretty much the same values for predicted number of medals (differences likely to be more visible when number of medals is low)
* half of the models have average deviation from the real number of total medals won of 2 medals



* compare predicted and actual total medals using 2014 Winter Olympics

```{r}
winter_2014 <- medals_winter %>%
  filter(year == "2014")

pred_2014 <- predict(fit_w2, winter_2014) %>%
  bind_cols(winter_2014) %>%
  mutate(.pred = round(.pred)) %>%
  mutate(.pred = if_else(.pred < 0, 0, .pred)) %>%     
  select(noc, total_medals, .pred) %>%          
  arrange(desc(total_medals)) %>%
  mutate(pct_error = ((abs(total_medals - .pred))/total_medals) * 100)

pred_2014 %>% slice_head(n = 10)

pred_2014 %>%
  ggplot(mapping = aes(x = .pred, y = total_medals)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(
    title = "Actual vs Predicted Total Medals",
    x = "Actual Medals",
    y = "Predicted Medals"
  ) +
  theme_minimal()

pred_2014 %>%
  filter(total_medals > 10) %>%
  summarise(avg_error = mean(pct_error)) 

rmse(pred_2014, truth = total_medals, estimate = .pred)
mae(pred_2014, truth = total_medals, estimate = .pred)

pred_2014 %>%
  summarise(avg = mean(total_medals))


results_winter %>%
  filter(mae < 2)%>%
  count()
```

* average error for teams with more than 10 medals is 12.4%
* rmse is 2.9, or 12.6% of mean (23.0), which is a worse fit than for Summer Olympics











### Some Conclusions

* number of athletes, as expected, is present in all models with MAE = 2 to 1 sf
* models without num_athletes have rmse of at least 18 for Summer and 7 for Winter Olympics
* less data for Winter Olympics (started in 1924) 
* a lot of missing data for early Games, some values (age, height, weight) substituted for average values for that year and season
* medals won before - issues with countries like Soviet Union, 	
Czechoslovakia, East/West Germany



